{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12719392, 0.07602623],\n",
       "       [0.42571454, 0.44914012],\n",
       "       [0.05950468, 0.05146188],\n",
       "       [0.31472534, 0.23648752],\n",
       "       [0.061582  , 0.17775878],\n",
       "       [0.26516871, 0.21099376],\n",
       "       [0.07276815, 0.2966436 ],\n",
       "       [0.12780171, 0.40894899],\n",
       "       [0.26000584, 0.40420933],\n",
       "       [0.08128355, 0.3214042 ],\n",
       "       [0.13140583, 0.42142161],\n",
       "       [0.24875974, 0.23672868],\n",
       "       [0.38391742, 0.36774915],\n",
       "       [0.08721717, 0.1845068 ],\n",
       "       [0.41734677, 0.38446297],\n",
       "       [0.2489276 , 0.12474914],\n",
       "       [0.07660528, 0.13836602],\n",
       "       [0.28749524, 0.08352194],\n",
       "       [0.2930273 , 0.19112219],\n",
       "       [0.3471332 , 0.09847199],\n",
       "       [0.32795873, 0.08230438],\n",
       "       [0.21339041, 0.16101675],\n",
       "       [0.36194694, 0.01171719],\n",
       "       [0.32423601, 0.47362081],\n",
       "       [0.45483277, 0.01995599],\n",
       "       [0.1326681 , 0.4759635 ],\n",
       "       [0.2183375 , 0.2953861 ],\n",
       "       [0.20994714, 0.10284431],\n",
       "       [0.26467161, 0.10438666],\n",
       "       [0.01780976, 0.11341754],\n",
       "       [0.47475233, 0.24430455],\n",
       "       [0.46949407, 0.15926973],\n",
       "       [0.16077959, 0.47631171],\n",
       "       [0.38423407, 0.22620246],\n",
       "       [0.01139099, 0.43505947],\n",
       "       [0.28892671, 0.31017625],\n",
       "       [0.28501663, 0.25176404],\n",
       "       [0.48073906, 0.38017667],\n",
       "       [0.32171   , 0.29383229],\n",
       "       [0.07659494, 0.03467376],\n",
       "       [0.29562832, 0.22649387],\n",
       "       [0.10468628, 0.04012506],\n",
       "       [0.31157844, 0.02769032],\n",
       "       [0.45360235, 0.20352081],\n",
       "       [0.15312886, 0.15451937],\n",
       "       [0.1801559 , 0.38458892],\n",
       "       [0.29870841, 0.24900004],\n",
       "       [0.34413892, 0.07067624],\n",
       "       [0.31618773, 0.40300447],\n",
       "       [0.33511669, 0.24750832]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "inputs = np.array([[random()/2 for i in range(2)] for j in range(1000)])\n",
    "inputs[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.15485921, -0.87631017, -1.00534223,  0.64501655,  0.81029146,\n",
       "          0.95863109, -1.09586384],\n",
       "        [ 0.05261376,  0.86886353, -1.09540752,  0.79323226, -1.14472996,\n",
       "          0.08420204, -1.01230927]]), array([[ 0.41871381],\n",
       "        [-0.44755161],\n",
       "        [-0.71574782],\n",
       "        [-0.34354989],\n",
       "        [-0.32074018],\n",
       "        [-1.54785278],\n",
       "        [-1.00916125]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [2,7,1]\n",
    "weight = []\n",
    "for i in range(len(layers)-1):\n",
    "    w = np.random.randn(layers[i] , layers[i+1] )\n",
    "    #w = np.array([[random()/2 for _ in range(layers[i+1])] for _ in range(layers[i])])\n",
    "    weight.append(w)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0002711767200777975 at Epoch 1\n",
      "Error: 0.0002711782242260903 at Epoch 2\n",
      "Error: 0.0002711797140411572 at Epoch 3\n",
      "Error: 0.0002711811894543877 at Epoch 4\n",
      "Error: 0.00027118265041275704 at Epoch 5\n",
      "Error: 0.00027118409687776037 at Epoch 6\n",
      "Error: 0.0002711855288244031 at Epoch 7\n",
      "Error: 0.00027118694624025865 at Epoch 8\n",
      "Error: 0.0002711883491245839 at Epoch 9\n",
      "Error: 0.00027118973748748565 at Epoch 10\n",
      "Error: 0.000271191111349144 at Epoch 11\n",
      "Error: 0.00027119247073907886 at Epoch 12\n",
      "Error: 0.0002711938156954606 at Epoch 13\n",
      "Error: 0.0002711951462644716 at Epoch 14\n",
      "Error: 0.00027119646249969785 at Epoch 15\n",
      "Error: 0.0002711977644615628 at Epoch 16\n",
      "Error: 0.00027119905221679585 at Epoch 17\n",
      "Error: 0.0002712003258379338 at Epoch 18\n",
      "Error: 0.0002712015854028522 at Epoch 19\n",
      "Error: 0.00027120283099432984 at Epoch 20\n",
      "Error: 0.00027120406269963383 at Epoch 21\n",
      "Error: 0.0002712052806101364 at Epoch 22\n",
      "Error: 0.00027120648482095613 at Epoch 23\n",
      "Error: 0.0002712076754306148 at Epoch 24\n",
      "Error: 0.00027120885254072486 at Epoch 25\n",
      "Error: 0.00027121001625568996 at Epoch 26\n",
      "Error: 0.00027121116668243016 at Epoch 27\n",
      "Error: 0.00027121230393011865 at Epoch 28\n",
      "Error: 0.00027121342810994086 at Epoch 29\n",
      "Error: 0.00027121453933486505 at Epoch 30\n",
      "Error: 0.00027121563771943206 at Epoch 31\n",
      "Error: 0.00027121672337955327 at Epoch 32\n",
      "Error: 0.0002712177964323246 at Epoch 33\n",
      "Error: 0.0002712188569958577 at Epoch 34\n",
      "Error: 0.00027121990518911063 at Epoch 35\n",
      "Error: 0.0002712209411317433 at Epoch 36\n",
      "Error: 0.0002712219649439695 at Epoch 37\n",
      "Error: 0.00027122297674643254 at Epoch 38\n",
      "Error: 0.0002712239766600749 at Epoch 39\n",
      "Error: 0.000271224964806031 at Epoch 40\n",
      "Error: 0.0002712259413055157 at Epoch 41\n",
      "Error: 0.00027122690627972825 at Epoch 42\n",
      "Error: 0.00027122785984975864 at Epoch 43\n",
      "Error: 0.000271228802136505 at Epoch 44\n",
      "Error: 0.00027122973326059043 at Epoch 45\n",
      "Error: 0.0002712306533422933 at Epoch 46\n",
      "Error: 0.00027123156250147675 at Epoch 47\n",
      "Error: 0.0002712324608575304 at Epoch 48\n",
      "Error: 0.00027123334852930265 at Epoch 49\n",
      "Error: 0.0002712342256350603 at Epoch 50\n",
      "Error: 0.00027123509229243085 at Epoch 51\n",
      "Error: 0.00027123594861835443 at Epoch 52\n",
      "Error: 0.0002712367947290528 at Epoch 53\n",
      "Error: 0.0002712376307399827 at Epoch 54\n",
      "Error: 0.00027123845676580365 at Epoch 55\n",
      "Error: 0.00027123927292034816 at Epoch 56\n",
      "Error: 0.0002712400793165882 at Epoch 57\n",
      "Error: 0.00027124087606661596 at Epoch 58\n",
      "Error: 0.0002712416632816127 at Epoch 59\n",
      "Error: 0.00027124244107183345 at Epoch 60\n",
      "Error: 0.00027124320954658614 at Epoch 61\n",
      "Error: 0.00027124396881421175 at Epoch 62\n",
      "Error: 0.00027124471898207686 at Epoch 63\n",
      "Error: 0.0002712454601565525 at Epoch 64\n",
      "Error: 0.00027124619244300706 at Epoch 65\n",
      "Error: 0.00027124691594579566 at Epoch 66\n",
      "Error: 0.0002712476307682504 at Epoch 67\n",
      "Error: 0.0002712483370126786 at Epoch 68\n",
      "Error: 0.00027124903478035074 at Epoch 69\n",
      "Error: 0.0002712497241714993 at Epoch 70\n",
      "Error: 0.0002712504052853149 at Epoch 71\n",
      "Error: 0.0002712510782199436 at Epoch 72\n",
      "Error: 0.00027125174307248914 at Epoch 73\n",
      "Error: 0.00027125239993900697 at Epoch 74\n",
      "Error: 0.00027125304891451057 at Epoch 75\n",
      "Error: 0.0002712536900929672 at Epoch 76\n",
      "Error: 0.0002712543235673051 at Epoch 77\n",
      "Error: 0.00027125494942941276 at Epoch 78\n",
      "Error: 0.0002712555677701433 at Epoch 79\n",
      "Error: 0.0002712561786793206 at Epoch 80\n",
      "Error: 0.0002712567822457401 at Epoch 81\n",
      "Error: 0.0002712573785571762 at Epoch 82\n",
      "Error: 0.0002712579677003885 at Epoch 83\n",
      "Error: 0.00027125854976112413 at Epoch 84\n",
      "Error: 0.00027125912482413013 at Epoch 85\n",
      "Error: 0.0002712596929731521 at Epoch 86\n",
      "Error: 0.0002712602542909495 at Epoch 87\n",
      "Error: 0.00027126080885929546 at Epoch 88\n",
      "Error: 0.00027126135675899167 at Epoch 89\n",
      "Error: 0.0002712618980698686 at Epoch 90\n",
      "Error: 0.00027126243287080065 at Epoch 91\n",
      "Error: 0.000271262961239709 at Epoch 92\n",
      "Error: 0.00027126348325357365 at Epoch 93\n",
      "Error: 0.00027126399898843835 at Epoch 94\n",
      "Error: 0.00027126450851942347 at Epoch 95\n",
      "Error: 0.0002712650119207318 at Epoch 96\n",
      "Error: 0.00027126550926565913 at Epoch 97\n",
      "Error: 0.00027126600062660276 at Epoch 98\n",
      "Error: 0.0002712664860750692 at Epoch 99\n",
      "Error: 0.0002712669656816868 at Epoch 100\n",
      "Withoud Batch Our Neural Network Shows: 0.3 + 0.4 = 0.7155280001891051\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stochastic Training of Neural Networks via\n",
    "Successive Convex Approximation\n",
    "'''\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,5], num_outputs=3, weight = 0):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights = weight\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        '''\n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1] )\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        # print(\"weights: \", weights)\n",
    "        '''\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "        # print(\"activations: \", activations)\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        # print(\"derivatives: \", derivatives)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            # print(\"net inputs:\", net_inputs)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "            # print(\"activations: {} \".format(activations))\n",
    "            # print(\"final activations:\", activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_prop(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            #print(\"activations:\", activations)\n",
    "            # delta:= error.dedrivative_of_activation_function\n",
    "            delta = error * self.tanh_derivative(activations)\n",
    "            #print(\"delta: \", delta)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            #print(\"delta reshaped:\" , delta_reshaped)\n",
    "            current_activations = self.activations[i]\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)\n",
    "            #print(\"current activations:\", current_activations)\n",
    "            #print(\"current activations reshaped\", current_activations_reshaped)\n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            # error := error. derivative_of_activation_function.weights\n",
    "            # delta := error. derivative_of_activation_function\n",
    "            # error := delta.weights\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            #print(error)\n",
    "            #print(\"derivatives:\", self.derivatives[i])\n",
    "        return error\n",
    "\n",
    "    def training(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for input, target in zip(inputs, targets):\n",
    "\n",
    "                output = self.forward(input)\n",
    "                error = output - target\n",
    "                self.back_prop(error)\n",
    "                self.gradient_descent(learning_rate)\n",
    "                sum_error += self.mean_square_error(target, output)\n",
    "            print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "\n",
    "    def mean_square_error(self, target, output):\n",
    "        return np.average((output - target)**2)\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights -= derivatives * learning_rate \n",
    "\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return (1-x**2)\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP(2,[7],1, weight)\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp.training(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"Withoud Batch Our Neural Network Shows: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Batch Our Neural Network Shoews: 0.3 + 0.5 = 0.7920696714836456\n"
     ]
    }
   ],
   "source": [
    "pre_input = np.array([0.3, 0.5])\n",
    "prediction = mlp.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "print(\"Without Batch Our Neural Network Shoews: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.6552613527915124e-05 at Epoch 1\n",
      "Error: 1.6517013689356568e-05 at Epoch 2\n",
      "Error: 1.6501963565660943e-05 at Epoch 3\n",
      "Error: 1.649663647180579e-05 at Epoch 4\n",
      "Error: 1.6497397947959167e-05 at Epoch 5\n",
      "Error: 1.650146876418067e-05 at Epoch 6\n",
      "Error: 1.6507072275678335e-05 at Epoch 7\n",
      "Error: 1.6513142093019418e-05 at Epoch 8\n",
      "Error: 1.6519071606730672e-05 at Epoch 9\n",
      "Error: 1.652454275947671e-05 at Epoch 10\n",
      "Error: 1.652941391071914e-05 at Epoch 11\n",
      "Error: 1.6533647671644207e-05 at Epoch 12\n",
      "Error: 1.6537265175545626e-05 at Epoch 13\n",
      "Error: 1.65403176350816e-05 at Epoch 14\n",
      "Error: 1.6542869085451796e-05 at Epoch 15\n",
      "Error: 1.6544986279279207e-05 at Epoch 16\n",
      "Error: 1.654673308712629e-05 at Epoch 17\n",
      "Error: 1.6548167683195442e-05 at Epoch 18\n",
      "Error: 1.6549341408716128e-05 at Epoch 19\n",
      "Error: 1.655029860855895e-05 at Epoch 20\n",
      "Error: 1.6551076999613694e-05 at Epoch 21\n",
      "Error: 1.6551708299652733e-05 at Epoch 22\n",
      "Error: 1.65522189544513e-05 at Epoch 23\n",
      "Error: 1.65526308699691e-05 at Epoch 24\n",
      "Error: 1.655296209945126e-05 at Epoch 25\n",
      "Error: 1.6553227461607562e-05 at Epoch 26\n",
      "Error: 1.6553439081643317e-05 at Epoch 27\n",
      "Error: 1.655360685579276e-05 at Epoch 28\n",
      "Error: 1.6553738844703167e-05 at Epoch 29\n",
      "Error: 1.6553841603172185e-05 at Epoch 30\n",
      "Error: 1.6553920454400463e-05 at Epoch 31\n",
      "Error: 1.655397971673829e-05 at Epoch 32\n",
      "Error: 1.6554022890288505e-05 at Epoch 33\n",
      "Error: 1.655405280991043e-05 at Epoch 34\n",
      "Error: 1.6554071770315237e-05 at Epoch 35\n",
      "Error: 1.6554081628105267e-05 at Epoch 36\n",
      "Error: 1.655408388485674e-05 at Epoch 37\n",
      "Error: 1.6554079754664117e-05 at Epoch 38\n",
      "Error: 1.6554070218989106e-05 at Epoch 39\n",
      "Error: 1.6554056071156372e-05 at Epoch 40\n",
      "Error: 1.6554037952421687e-05 at Epoch 41\n",
      "Error: 1.655401638118969e-05 at Epoch 42\n",
      "Error: 1.6553991776671723e-05 at Epoch 43\n",
      "Error: 1.6553964478029626e-05 at Epoch 44\n",
      "Error: 1.655393475986597e-05 at Epoch 45\n",
      "Error: 1.655390284474902e-05 at Epoch 46\n",
      "Error: 1.6553868913342007e-05 at Epoch 47\n",
      "Error: 1.6553833112588742e-05 at Epoch 48\n",
      "Error: 1.655379556232929e-05 at Epoch 49\n",
      "Error: 1.6553756360645718e-05 at Epoch 50\n",
      "Error: 1.6553715588178867e-05 at Epoch 51\n",
      "Error: 1.6553673311614514e-05 at Epoch 52\n",
      "Error: 1.6553629586497938e-05 at Epoch 53\n",
      "Error: 1.655358445950392e-05 at Epoch 54\n",
      "Error: 1.6553537970268775e-05 at Epoch 55\n",
      "Error: 1.655349015286609e-05 at Epoch 56\n",
      "Error: 1.6553441036995724e-05 at Epoch 57\n",
      "Error: 1.6553390648940753e-05 at Epoch 58\n",
      "Error: 1.655333901233607e-05 at Epoch 59\n",
      "Error: 1.6553286148785936e-05 at Epoch 60\n",
      "Error: 1.6553232078357995e-05 at Epoch 61\n",
      "Error: 1.6553176819978688e-05 at Epoch 62\n",
      "Error: 1.6553120391748168e-05 at Epoch 63\n",
      "Error: 1.655306281119e-05 at Epoch 64\n",
      "Error: 1.655300409544866e-05 at Epoch 65\n",
      "Error: 1.6552944261444975e-05 at Epoch 66\n",
      "Error: 1.655288332599678e-05 at Epoch 67\n",
      "Error: 1.655282130591173e-05 at Epoch 68\n",
      "Error: 1.655275821805816e-05 at Epoch 69\n",
      "Error: 1.655269407941778e-05 at Epoch 70\n",
      "Error: 1.6552628907124312e-05 at Epoch 71\n",
      "Error: 1.6552562718488854e-05 at Epoch 72\n",
      "Error: 1.65524955310177e-05 at Epoch 73\n",
      "Error: 1.6552427362421446e-05 at Epoch 74\n",
      "Error: 1.655235823061878e-05 at Epoch 75\n",
      "Error: 1.6552288153734564e-05 at Epoch 76\n",
      "Error: 1.6552217150095374e-05 at Epoch 77\n",
      "Error: 1.6552145238220505e-05 at Epoch 78\n",
      "Error: 1.6552072436812137e-05 at Epoch 79\n",
      "Error: 1.6551998764743037e-05 at Epoch 80\n",
      "Error: 1.6551924241043065e-05 at Epoch 81\n",
      "Error: 1.6551848884883347e-05 at Epoch 82\n",
      "Error: 1.6551772715563354e-05 at Epoch 83\n",
      "Error: 1.6551695752493326e-05 at Epoch 84\n",
      "Error: 1.655161801517865e-05 at Epoch 85\n",
      "Error: 1.6551539523204353e-05 at Epoch 86\n",
      "Error: 1.655146029621856e-05 at Epoch 87\n",
      "Error: 1.6551380353916804e-05 at Epoch 88\n",
      "Error: 1.6551299716026304e-05 at Epoch 89\n",
      "Error: 1.6551218402290266e-05 at Epoch 90\n",
      "Error: 1.6551136432453413e-05 at Epoch 91\n",
      "Error: 1.6551053826246586e-05 at Epoch 92\n",
      "Error: 1.6550970603373097e-05 at Epoch 93\n",
      "Error: 1.655088678349467e-05 at Epoch 94\n",
      "Error: 1.6550802386217428e-05 at Epoch 95\n",
      "Error: 1.655071743108033e-05 at Epoch 96\n",
      "Error: 1.6550631937541645e-05 at Epoch 97\n",
      "Error: 1.6550545924967676e-05 at Epoch 98\n",
      "Error: 1.6550459412621006e-05 at Epoch 99\n",
      "Error: 1.6550372419649307e-05 at Epoch 100\n",
      "Using Batch Our Neural network Shows that 0.3 + 0.4 = 0.6996148879950501\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stochastic Training of Neural Networks via\n",
    "Successive Convex Approximation\n",
    "'''\n",
    "import numpy as np\n",
    "from random import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class MLP_B():\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,5], num_outputs=3, weight = 0):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights = weight\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        '''\n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1] )\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        # print(\"weights: \", weights)\n",
    "        '''\n",
    "        \n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "        # print(\"activations: \", activations)\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        # print(\"derivatives: \", derivatives)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            # print(\"net inputs:\", net_inputs)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "            # print(\"activations: {} \".format(activations))\n",
    "            # print(\"final activations:\", activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_prop(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            #print(\"activations:\", activations)\n",
    "            # delta:= error.dedrivative_of_activation_function\n",
    "            delta = error * self.tanh_derivative(activations)\n",
    "            #print(\"delta: \", delta)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            #print(\"delta reshaped:\" , delta_reshaped)\n",
    "            current_activations = self.activations[i]\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)\n",
    "            #print(\"current activations:\", current_activations)\n",
    "            #print(\"current activations reshaped\", current_activations_reshaped)\n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            # error := error. derivative_of_activation_function.weights\n",
    "            # delta := error. derivative_of_activation_function\n",
    "            # error := delta.weights\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            #print(error)\n",
    "            #print(\"derivatives:\", self.derivatives[i])\n",
    "        return error\n",
    "\n",
    "    def training(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            #sum_error = 0\n",
    "            batch_size = 100\n",
    "            for step in range(int(len(inputs)/batch_size)):\n",
    "                sum_error = 0\n",
    "                first_index = 0\n",
    "                last_index = batch_size\n",
    "                batch_input = inputs[first_index: last_index]\n",
    "                batch_target = targets[first_index: last_index]\n",
    "\n",
    "                for input, target in zip(batch_input, batch_target):\n",
    "                    output = self.forward(input)\n",
    "                    error = output - target\n",
    "                    self.back_prop(error)\n",
    "                    self.gradient_descent(learning_rate)\n",
    "                    sum_error += self.mean_square_error(target, output)\n",
    "                    \n",
    "                first_index = last_index\n",
    "                last_index += batch_size \n",
    "                #print(\"Error: {} at batch {}\".format(sum_error / batch_size, step+1))\n",
    "               \n",
    "                \n",
    "  \n",
    "            print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "            #plt.plot(sum_error)\n",
    "\n",
    "    def mean_square_error(self, target, output):\n",
    "        return np.average((output - target)**2)\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights -= derivatives * learning_rate \n",
    "\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return (1-x**2)\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp_b = MLP_B(2,[7],1 ,weight )\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp_b.training(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp_b.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"Using Batch Our Neural network Shows that {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Batch Our Neural network Shows that 0.3 + 0.3 = 0.6059366340992841\n"
     ]
    }
   ],
   "source": [
    "pre_input = np.array([0.30,  0.3])\n",
    "prediction_b = mlp_b.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "print(\"With Batch Our Neural network Shows that {} + {} = {}\".format(pre_input[0], pre_input[1], prediction_b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without Batch Our Neural Network Shows: 0.3 + 0.4 = 0.7155102131769498\n",
      "Using Batch Our Neural Network Shows: 0.3 + 0.4 = 0.6996198784340236\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stochastic Training of Neural Networks via\n",
    "Successive Convex Approximation\n",
    "'''\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,5], num_outputs=3, weight = 0):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights = weight\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        '''\n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1] )\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        # print(\"weights: \", weights)\n",
    "        '''\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "        # print(\"activations: \", activations)\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        # print(\"derivatives: \", derivatives)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            # print(\"net inputs:\", net_inputs)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "            # print(\"activations: {} \".format(activations))\n",
    "            # print(\"final activations:\", activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_prop(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            #print(\"activations:\", activations)\n",
    "            # delta:= error.dedrivative_of_activation_function\n",
    "            delta = error * self.tanh_derivative(activations)\n",
    "            #print(\"delta: \", delta)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            #print(\"delta reshaped:\" , delta_reshaped)\n",
    "            current_activations = self.activations[i]\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)\n",
    "            #print(\"current activations:\", current_activations)\n",
    "            #print(\"current activations reshaped\", current_activations_reshaped)\n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            # error := error. derivative_of_activation_function.weights\n",
    "            # delta := error. derivative_of_activation_function\n",
    "            # error := delta.weights\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            #print(error)\n",
    "            #print(\"derivatives:\", self.derivatives[i])\n",
    "        return error\n",
    "\n",
    "    def training(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for input, target in zip(inputs, targets):\n",
    "\n",
    "                output = self.forward(input)\n",
    "                error = output - target\n",
    "                self.back_prop(error)\n",
    "                self.gradient_descent(learning_rate)\n",
    "                sum_error += self.mean_square_error(target, output)\n",
    "            #print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "    \n",
    "    def training_batch(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            #sum_error = 0\n",
    "            batch_size = 100\n",
    "            for step in range(int(len(inputs)/batch_size)):\n",
    "                sum_error = 0\n",
    "                first_index = 0\n",
    "                last_index = batch_size\n",
    "                batch_input = inputs[first_index: last_index]\n",
    "                batch_target = targets[first_index: last_index]\n",
    "\n",
    "                for input, target in zip(batch_input, batch_target):\n",
    "                    output = self.forward(input)\n",
    "                    error = output - target\n",
    "                    self.back_prop(error)\n",
    "                    self.gradient_descent(learning_rate)\n",
    "                    sum_error += self.mean_square_error(target, output)\n",
    "                    \n",
    "                first_index = last_index\n",
    "                last_index += batch_size \n",
    "                #print(\"Error: {} at batch {}\".format(sum_error / batch_size, step+1))\n",
    "               \n",
    "                \n",
    "  \n",
    "            #print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "            #plt.plot(sum_error)\n",
    "\n",
    "    def mean_square_error(self, target, output):\n",
    "        return np.average((output - target)**2)\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights -= derivatives * learning_rate \n",
    "\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return (1-x**2)\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP(2,[7],1, weight)\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp.training(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"without Batch Our Neural Network Shows: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "    mlp_b = MLP(2,[7],1 ,weight )\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp_b.training_batch(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp_b.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"Using Batch Our Neural Network Shows: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data_SCA.csv\")\n",
    "\n",
    "np.random.seed(1)\n",
    "m = np.zeros(7)\n",
    "type(df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=[3]\n",
    "m=[3,5]\n",
    "p=[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "#layers = [n]+m+[p]\n",
    "layers = n+m+p\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "data = [ [2,  3,  5 ],\n",
    "        [3, 4, 7],\n",
    "        [3, 6, 9 ],\n",
    "       [1, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the network input is: [ 0.83898341  0.93110208  0.28558733  0.88514116 -0.75439794  1.25286816\n",
      "  0.51292982]\n",
      "the network input is: [0.37622306 0.99751563]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "\n",
    "    def __init__(self, num_inputs=7, num_hidden=[3,5], num_outputs=2):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        self.weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i], layers[i+1])\n",
    "            self.weights.append(w)\n",
    "\n",
    "    def forward_propagate(self,inputs):\n",
    "        activations = inputs\n",
    "        for w in self.weights:\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            activations = self.sigmoid(net_inputs)\n",
    "        return activations\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP()\n",
    "    inputs = np.random.randn(mlp.num_inputs)\n",
    "    outputs = mlp.forward_propagate(inputs)\n",
    "    print(\"the network input is: {}\" .format(inputs)) \n",
    "    print(\"the network input is: {}\" .format(outputs)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9a2f5e33c135>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print (n)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "n = np.random.randn(10)\n",
    "#print (n)\n",
    "plt.plot(n, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15360698, 0.11081771],\n",
       "       [0.01687841, 0.39278484],\n",
       "       [0.37963109, 0.37671021],\n",
       "       ...,\n",
       "       [0.03589354, 0.18585574],\n",
       "       [0.22705925, 0.1523342 ],\n",
       "       [0.12052998, 0.3102365 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.0002650680037038177 at Epoch 1\n",
      "Error: 0.00026498336062642086 at Epoch 2\n",
      "Error: 0.00026489952626471895 at Epoch 3\n",
      "Error: 0.00026481649125714534 at Epoch 4\n",
      "Error: 0.0002647342464086478 at Epoch 5\n",
      "Error: 0.000264652782681403 at Epoch 6\n",
      "Error: 0.0002645720911862581 at Epoch 7\n",
      "Error: 0.0002644921631748544 at Epoch 8\n",
      "Error: 0.0002644129900323846 at Epoch 9\n",
      "Error: 0.00026433456327093886 at Epoch 10\n",
      "Error: 0.0002642568745234132 at Epoch 11\n",
      "Error: 0.00026417991553792034 at Epoch 12\n",
      "Error: 0.00026410367817268653 at Epoch 13\n",
      "Error: 0.0002640281543913957 at Epoch 14\n",
      "Error: 0.00026395333625893973 at Epoch 15\n",
      "Error: 0.00026387921593755734 at Epoch 16\n",
      "Error: 0.00026380578568332985 at Epoch 17\n",
      "Error: 0.00026373303784300406 at Epoch 18\n",
      "Error: 0.0002636609648511177 at Epoch 19\n",
      "Error: 0.0002635895592274073 at Epoch 20\n",
      "Error: 0.00026351881357447517 at Epoch 21\n",
      "Error: 0.0002634487205756929 at Epoch 22\n",
      "Error: 0.0002633792729933222 at Epoch 23\n",
      "Error: 0.00026331046366683534 at Epoch 24\n",
      "Error: 0.000263242285511418 at Epoch 25\n",
      "Error: 0.0002631747315166387 at Epoch 26\n",
      "Error: 0.00026310779474526493 at Epoch 27\n",
      "Error: 0.0002630414683322247 at Epoch 28\n",
      "Error: 0.00026297574548367407 at Epoch 29\n",
      "Error: 0.00026291061947619895 at Epoch 30\n",
      "Error: 0.0002628460836560898 at Epoch 31\n",
      "Error: 0.0002627821314387259 at Epoch 32\n",
      "Error: 0.0002627187563080209 at Epoch 33\n",
      "Error: 0.00026265595181595053 at Epoch 34\n",
      "Error: 0.0002625937115821274 at Epoch 35\n",
      "Error: 0.0002625320292934371 at Epoch 36\n",
      "Error: 0.0002624708987037137 at Epoch 37\n",
      "Error: 0.00026241031363345295 at Epoch 38\n",
      "Error: 0.00026235026796955383 at Epoch 39\n",
      "Error: 0.0002622907556650921 at Epoch 40\n",
      "Error: 0.00026223177073910977 at Epoch 41\n",
      "Error: 0.0002621733072764137 at Epoch 42\n",
      "Error: 0.00026211535942739534 at Epoch 43\n",
      "Error: 0.0002620579214078492 at Epoch 44\n",
      "Error: 0.0002620009874988002 at Epoch 45\n",
      "Error: 0.00026194455204631975 at Epoch 46\n",
      "Error: 0.00026188860946135475 at Epoch 47\n",
      "Error: 0.00026183315421953666 at Epoch 48\n",
      "Error: 0.0002617781808609889 at Epoch 49\n",
      "Error: 0.0002617236839901237 at Epoch 50\n",
      "Error: 0.0002616696582754292 at Epoch 51\n",
      "Error: 0.0002616160984492343 at Epoch 52\n",
      "Error: 0.0002615629993074721 at Epoch 53\n",
      "Error: 0.00026151035570941647 at Epoch 54\n",
      "Error: 0.00026145816257741193 at Epoch 55\n",
      "Error: 0.0002614064148965746 at Epoch 56\n",
      "Error: 0.0002613551077144901 at Epoch 57\n",
      "Error: 0.0002613042361408789 at Epoch 58\n",
      "Error: 0.00026125379534725835 at Epoch 59\n",
      "Error: 0.00026120378056657264 at Epoch 60\n",
      "Error: 0.0002611541870928107 at Epoch 61\n",
      "Error: 0.0002611050102806143 at Epoch 62\n",
      "Error: 0.0002610562455448543 at Epoch 63\n",
      "Error: 0.0002610078883601926 at Epoch 64\n",
      "Error: 0.00026095993426064065 at Epoch 65\n",
      "Error: 0.0002609123788390816 at Epoch 66\n",
      "Error: 0.0002608652177467931 at Epoch 67\n",
      "Error: 0.000260818446692943 at Epoch 68\n",
      "Error: 0.0002607720614440794 at Epoch 69\n",
      "Error: 0.0002607260578235964 at Epoch 70\n",
      "Error: 0.00026068043171120146 at Epoch 71\n",
      "Error: 0.0002606351790423527 at Epoch 72\n",
      "Error: 0.00026059029580769675 at Epoch 73\n",
      "Error: 0.0002605457780524919 at Epoch 74\n",
      "Error: 0.0002605016218760188 at Epoch 75\n",
      "Error: 0.00026045782343098536 at Epoch 76\n",
      "Error: 0.0002604143789229177 at Epoch 77\n",
      "Error: 0.00026037128460954943 at Epoch 78\n",
      "Error: 0.00026032853680019836 at Epoch 79\n",
      "Error: 0.0002602861318551391 at Epoch 80\n",
      "Error: 0.0002602440661849697 at Epoch 81\n",
      "Error: 0.00026020233624997417 at Epoch 82\n",
      "Error: 0.000260160938559478 at Epoch 83\n",
      "Error: 0.0002601198696712025 at Epoch 84\n",
      "Error: 0.00026007912619061674 at Epoch 85\n",
      "Error: 0.0002600387047702854 at Epoch 86\n",
      "Error: 0.000259998602109215 at Epoch 87\n",
      "Error: 0.00025995881495220356 at Epoch 88\n",
      "Error: 0.00025991934008918343 at Epoch 89\n",
      "Error: 0.00025988017435457037 at Epoch 90\n",
      "Error: 0.0002598413146266076 at Epoch 91\n",
      "Error: 0.000259802757826721 at Epoch 92\n",
      "Error: 0.0002597645009188621 at Epoch 93\n",
      "Error: 0.00025972654090886983 at Epoch 94\n",
      "Error: 0.0002596888748438201 at Epoch 95\n",
      "Error: 0.00025965149981139096 at Epoch 96\n",
      "Error: 0.0002596144129392243 at Epoch 97\n",
      "Error: 0.0002595776113942928 at Epoch 98\n",
      "Error: 0.0002595410923822772 at Epoch 99\n",
      "Error: 0.0002595048531469372 at Epoch 100\n",
      "0.3 + 0.2 = 0.5047049841364182\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "layers = [2,7,1] \n",
    "weight = []\n",
    "for i in range(len(layers)-1):\n",
    "    w = np.random.randn(layers[i] , layers[i+1] )\n",
    "    weight.append(w)\n",
    "'''  \n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP(2,[7],1,weight)\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp.training(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.2])\n",
    "    prediction = mlp.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"{} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "the network input is: [0.1851304  0.70187653 0.24035562]\n",
      "the network input is: [-0.6848136]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, num_inputs=2, num_hidden=[3,5], num_outputs=2):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            print (i)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "       \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "        \n",
    "        return activations\n",
    "    \n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP(3,[3, 4, 5],1)\n",
    "    inputs = np.random.rand(mlp.num_inputs)\n",
    "    outputs = mlp.forward(inputs)\n",
    "    print(\"the network input is: {}\" .format(inputs)) \n",
    "    print(\"the network input is: {}\" .format(outputs)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.95257413, 0.99330715])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tanh(x):\n",
    "    y = 1.0 / (1 + np.exp(-x))\n",
    "    return y\n",
    "x=np.array([1,3,5])\n",
    "y = tanh(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [3,5]\n",
    "a = a + [1] \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((2,3))\n",
    "a\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.89725804]\n",
      "[-5.53832495 -5.33909682 -4.3287109  -4.47419184 -5.83416573]\n",
      "[-5.78162053 -5.78622514 -5.9279111  -5.84073707]\n",
      "[-5.92575759 -5.2813184  -5.91963089]\n",
      "the network input is: [0.1 0.2 0.3]\n",
      "the network output is: [0.07200607]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,5], num_outputs=3):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1] )\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        # print(\"weights: \", weights)\n",
    "\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "        # print(\"activations: \", activations)\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        # print(\"derivatives: \", derivatives)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            # print(\"net inputs:\", net_inputs)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "            # print(\"activations: {} \".format(activations))\n",
    "            # print(\"final activations:\", activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_prop(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            delta = error * self.derivative(activations)\n",
    "            print(delta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return (1.0 - x**2)\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP(3,[3, 4, 5],1)\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    inputs = np.array([.1, .2, .3])\n",
    "    target = np.array([6])\n",
    "    outputs = mlp.forward(inputs)\n",
    "    error = outputs - target\n",
    "    mlp.back_prop(error)\n",
    "    print(\"the network input is: {}\" .format(inputs)) \n",
    "    print(\"the network output is: {}\" .format(outputs)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,5], [6,7], [9,0]])\n",
    "b = np.array([[1,2]])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.27, 0.36],\n",
       "       [0.3 , 0.34],\n",
       "       [0.28, 0.4 ],\n",
       "       ...,\n",
       "       [0.24, 0.19],\n",
       "       [0.29, 0.3 ],\n",
       "       [0.21, 0.38]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data_SCA.csv\")\n",
    "data = df.to_numpy()\n",
    "print(len(data))\n",
    "#train_data = data[: int(len(data)*0.7) ]\n",
    "#test_data = data[int(len(data)*0.7):  ]\n",
    "train_inputs =data[ : , : 2 ]\n",
    "train_targets =data[ : , -1:]\n",
    "train_targets\n",
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27,  0.36, 20.7 , ...,  3.  ,  0.45,  8.8 ],\n",
       "       [ 0.3 ,  0.34,  1.6 , ...,  3.3 ,  0.49,  9.5 ],\n",
       "       [ 0.28,  0.4 ,  6.9 , ...,  3.26,  0.44, 10.1 ],\n",
       "       ...,\n",
       "       [ 0.24,  0.19,  1.2 , ...,  2.99,  0.46,  9.4 ],\n",
       "       [ 0.29,  0.3 ,  1.1 , ...,  3.34,  0.38, 12.8 ],\n",
       "       [ 0.21,  0.38,  0.8 , ...,  3.26,  0.32, 11.8 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27,  0.36, 20.7 , ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 0.3 ,  0.34,  1.6 , ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 0.28,  0.4 ,  6.9 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 0.24,  0.19,  1.2 , ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 0.29,  0.3 ,  1.1 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 0.21,  0.38,  0.8 , ...,  0.32, 11.8 ,  6.  ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values[: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
