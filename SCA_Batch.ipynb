{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successive Convex Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Two different types of the python program.\n",
    "#1-Training without succesive batch.\n",
    "#2-Training with successive batch. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3703511 , 0.47352163],\n",
       "       [0.03080801, 0.28094676],\n",
       "       [0.34556038, 0.00361325],\n",
       "       [0.24914238, 0.35910704],\n",
       "       [0.2490735 , 0.27251771],\n",
       "       [0.21478062, 0.47246284],\n",
       "       [0.49179122, 0.00746667],\n",
       "       [0.21584044, 0.24365182],\n",
       "       [0.3628567 , 0.35567743],\n",
       "       [0.34362665, 0.01648451],\n",
       "       [0.211226  , 0.13968168],\n",
       "       [0.47604128, 0.30203129],\n",
       "       [0.45928186, 0.14746859],\n",
       "       [0.02900853, 0.20056201],\n",
       "       [0.49196311, 0.33029864],\n",
       "       [0.38040862, 0.18629717],\n",
       "       [0.23143768, 0.44584104],\n",
       "       [0.40152478, 0.37383465],\n",
       "       [0.26334988, 0.31957479],\n",
       "       [0.41483768, 0.28644245],\n",
       "       [0.06785831, 0.25617856],\n",
       "       [0.21668468, 0.22248738],\n",
       "       [0.2215131 , 0.47371242],\n",
       "       [0.10130332, 0.25744155],\n",
       "       [0.39926845, 0.17975317],\n",
       "       [0.37922717, 0.27359546],\n",
       "       [0.01657946, 0.36368681],\n",
       "       [0.37706797, 0.14695061],\n",
       "       [0.3043554 , 0.2358366 ],\n",
       "       [0.16008393, 0.16985629],\n",
       "       [0.49378233, 0.48690708],\n",
       "       [0.27280636, 0.49004949],\n",
       "       [0.34185645, 0.35508216],\n",
       "       [0.27231697, 0.02552542],\n",
       "       [0.07492943, 0.40406657],\n",
       "       [0.16132082, 0.03990907],\n",
       "       [0.17905752, 0.44676128],\n",
       "       [0.308182  , 0.10866464],\n",
       "       [0.4654324 , 0.46010742],\n",
       "       [0.13453191, 0.18969063],\n",
       "       [0.29108617, 0.3888874 ],\n",
       "       [0.27546896, 0.20008401],\n",
       "       [0.30942134, 0.34615319],\n",
       "       [0.25463349, 0.09517231],\n",
       "       [0.0348723 , 0.17023245],\n",
       "       [0.41413818, 0.1905673 ],\n",
       "       [0.07586632, 0.00817076],\n",
       "       [0.00902745, 0.1271583 ],\n",
       "       [0.10614982, 0.4670346 ],\n",
       "       [0.38385448, 0.47818499]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random selection Dataset generation for both program\n",
    "\n",
    "import numpy as np\n",
    "from random import random\n",
    "inputs = np.array([[random()/2 for i in range(2)] for j in range(1000)])\n",
    "inputs[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.7586089 , -0.42083518, -1.27812552,  0.94597251, -0.88909951,\n",
       "         -0.15487099, -0.10478637],\n",
       "        [ 1.08443659, -0.23778969, -1.09608714, -0.87859069,  0.0367101 ,\n",
       "         -0.74550382,  0.64938824]]), array([[ 0.57068241],\n",
       "        [-0.17589568],\n",
       "        [ 0.87764973],\n",
       "        [ 0.37338807],\n",
       "        [ 0.71101414],\n",
       "        [-0.69368014],\n",
       "        [-0.08083871]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating random weights for both the programs\n",
    "layers = [2,7,1]\n",
    "weight = []\n",
    "for i in range(len(layers)-1):\n",
    "    w = np.random.randn(layers[i] , layers[i+1] )\n",
    "    #w = np.array([[random()/2 for _ in range(layers[i+1])] for _ in range(layers[i])])\n",
    "    weight.append(w)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.004171230356743799 at Epoch 1\n",
      "Error: 0.0016566752242379024 at Epoch 2\n",
      "Error: 0.0010282145873621205 at Epoch 3\n",
      "Error: 0.0005474652030198103 at Epoch 4\n",
      "Error: 0.0004079193617664748 at Epoch 5\n",
      "Error: 0.00037905336908095657 at Epoch 6\n",
      "Error: 0.0003661384661497067 at Epoch 7\n",
      "Error: 0.0003571742852962757 at Epoch 8\n",
      "Error: 0.0003503181903758817 at Epoch 9\n",
      "Error: 0.00034482895105850017 at Epoch 10\n",
      "Error: 0.00034027738233582977 at Epoch 11\n",
      "Error: 0.00033639496391375286 at Epoch 12\n",
      "Error: 0.0003330068683947594 at Epoch 13\n",
      "Error: 0.0003299954404675235 at Epoch 14\n",
      "Error: 0.00032727914443663836 at Epoch 15\n",
      "Error: 0.00032479996577517204 at Epoch 16\n",
      "Error: 0.00032251562191875824 at Epoch 17\n",
      "Error: 0.0003203946057134842 at Epoch 18\n",
      "Error: 0.0003184129501152662 at Epoch 19\n",
      "Error: 0.00031655206817461277 at Epoch 20\n",
      "Error: 0.0003147972814818136 at Epoch 21\n",
      "Error: 0.0003131367991629749 at Epoch 22\n",
      "Error: 0.0003115609975730061 at Epoch 23\n",
      "Error: 0.0003100619042568179 at Epoch 24\n",
      "Error: 0.0003086328229230863 at Epoch 25\n",
      "Error: 0.00030726805720779934 at Epoch 26\n",
      "Error: 0.00030596270459486624 at Epoch 27\n",
      "Error: 0.0003047125007942783 at Epoch 28\n",
      "Error: 0.0003035137008430668 at Epoch 29\n",
      "Error: 0.0003023629872345325 at Epoch 30\n",
      "Error: 0.00030125739815419004 at Epoch 31\n",
      "Error: 0.0003001942708272957 at Epoch 32\n",
      "Error: 0.00029917119633644557 at Epoch 33\n",
      "Error: 0.00029818598322875765 at Epoch 34\n",
      "Error: 0.00029723662792138875 at Epoch 35\n",
      "Error: 0.0002963212904129383 at Epoch 36\n",
      "Error: 0.00029543827417253865 at Epoch 37\n",
      "Error: 0.00029458600934666125 at Epoch 38\n",
      "Error: 0.00029376303862279436 at Epoch 39\n",
      "Error: 0.00029296800523812217 at Epoch 40\n",
      "Error: 0.00029219964273365486 at Epoch 41\n",
      "Error: 0.0002914567661395565 at Epoch 42\n",
      "Error: 0.00029073826434266594 at Epoch 43\n",
      "Error: 0.00029004309343747567 at Epoch 44\n",
      "Error: 0.00028937027090082917 at Epoch 45\n",
      "Error: 0.0002887188704610367 at Epoch 46\n",
      "Error: 0.000288088017556088 at Epoch 47\n",
      "Error: 0.00028747688529454606 at Epoch 48\n",
      "Error: 0.00028688469084785565 at Epoch 49\n",
      "Error: 0.0002863106922148485 at Epoch 50\n",
      "Error: 0.00028575418530902424 at Epoch 51\n",
      "Error: 0.0002852145013270853 at Epoch 52\n",
      "Error: 0.0002846910043636669 at Epoch 53\n",
      "Error: 0.0002841830892425185 at Epoch 54\n",
      "Error: 0.0002836901795387493 at Epoch 55\n",
      "Error: 0.0002832117257703988 at Epoch 56\n",
      "Error: 0.00028274720374061126 at Epoch 57\n",
      "Error: 0.00028229611301423227 at Epoch 58\n",
      "Error: 0.0002818579755147741 at Epoch 59\n",
      "Error: 0.00028143233422951817 at Epoch 60\n",
      "Error: 0.0002810187520120344 at Epoch 61\n",
      "Error: 0.0002806168104727249 at Epoch 62\n",
      "Error: 0.0002802261089491091 at Epoch 63\n",
      "Error: 0.0002798462635485313 at Epoch 64\n",
      "Error: 0.0002794769062568038 at Epoch 65\n",
      "Error: 0.00027911768410701876 at Epoch 66\n",
      "Error: 0.00027876825840337746 at Epoch 67\n",
      "Error: 0.0002784283039954565 at Epoch 68\n",
      "Error: 0.00027809750859875896 at Epoch 69\n",
      "Error: 0.00027777557215787036 at Epoch 70\n",
      "Error: 0.00027746220624886577 at Epoch 71\n",
      "Error: 0.0002771571335179521 at Epoch 72\n",
      "Error: 0.00027686008715362414 at Epoch 73\n",
      "Error: 0.0002765708103898488 at Epoch 74\n",
      "Error: 0.0002762890560380355 at Epoch 75\n",
      "Error: 0.00027601458604573206 at Epoch 76\n",
      "Error: 0.000275747171080183 at Epoch 77\n",
      "Error: 0.00027548659013504806 at Epoch 78\n",
      "Error: 0.0002752326301586841 at Epoch 79\n",
      "Error: 0.00027498508570259676 at Epoch 80\n",
      "Error: 0.0002747437585887037 at Epoch 81\n",
      "Error: 0.000274508457594216 at Epoch 82\n",
      "Error: 0.00027427899815301454 at Epoch 83\n",
      "Error: 0.00027405520207247083 at Epoch 84\n",
      "Error: 0.00027383689726479275 at Epoch 85\n",
      "Error: 0.000273623917491965 at Epoch 86\n",
      "Error: 0.00027341610212350807 at Epoch 87\n",
      "Error: 0.0002732132959062639 at Epoch 88\n",
      "Error: 0.0002730153487455175 at Epoch 89\n",
      "Error: 0.00027282211549679565 at Epoch 90\n",
      "Error: 0.00027263345576772346 at Epoch 91\n",
      "Error: 0.00027244923372938905 at Epoch 92\n",
      "Error: 0.000272269317936651 at Epoch 93\n",
      "Error: 0.00027209358115692873 at Epoch 94\n",
      "Error: 0.00027192190020698686 at Epoch 95\n",
      "Error: 0.0002717541557972902 at Epoch 96\n",
      "Error: 0.0002715902323835217 at Epoch 97\n",
      "Error: 0.0002714300180248769 at Epoch 98\n",
      "Error: 0.0002712734042487935 at Epoch 99\n",
      "Error: 0.0002711202859217588 at Epoch 100\n",
      "Withoud Batch Our Neural Network Shows: 0.3 + 0.4 = 0.7200195520788946\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stochastic Training of Neural Networks without\n",
    "Successive Batch\n",
    "'''\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,5], num_outputs=3, weight = 0):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights = weight\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        '''\n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1] )\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        # print(\"weights: \", weights)\n",
    "        '''\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "        # print(\"activations: \", activations)\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        # print(\"derivatives: \", derivatives)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            # print(\"net inputs:\", net_inputs)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "            # print(\"activations: {} \".format(activations))\n",
    "            # print(\"final activations:\", activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_prop(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            #print(\"activations:\", activations)\n",
    "            # delta:= error.dedrivative_of_activation_function\n",
    "            delta = error * self.tanh_derivative(activations)\n",
    "            #print(\"delta: \", delta)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            #print(\"delta reshaped:\" , delta_reshaped)\n",
    "            current_activations = self.activations[i]\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)\n",
    "            #print(\"current activations:\", current_activations)\n",
    "            #print(\"current activations reshaped\", current_activations_reshaped)\n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            # error := error. derivative_of_activation_function.weights\n",
    "            # delta := error. derivative_of_activation_function\n",
    "            # error := delta.weights\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            #print(error)\n",
    "            #print(\"derivatives:\", self.derivatives[i])\n",
    "        return error\n",
    "\n",
    "    def training(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for input, target in zip(inputs, targets):\n",
    "\n",
    "                output = self.forward(input)\n",
    "                error = output - target\n",
    "                self.back_prop(error)\n",
    "                self.gradient_descent(learning_rate)\n",
    "                sum_error += self.mean_square_error(target, output)\n",
    "            print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "\n",
    "    def mean_square_error(self, target, output):\n",
    "        return np.average((output - target)**2)\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights -= derivatives * learning_rate \n",
    "\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return (1-x**2)\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP(2,[7],1, weight)\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp.training(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"Withoud Batch Our Neural Network Shows: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Batch Our Neural Network Shoews: 0.3 + 0.5 = 0.7931273966326561\n"
     ]
    }
   ],
   "source": [
    "pre_input = np.array([0.3, 0.5])\n",
    "prediction = mlp.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "print(\"Without Batch Our Neural Network Shoews: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4.1746689682785874e-05 at Epoch 1\n",
      "Error: 4.176025738073586e-05 at Epoch 2\n",
      "Error: 4.171603909105809e-05 at Epoch 3\n",
      "Error: 4.167097710731389e-05 at Epoch 4\n",
      "Error: 4.1630351646294155e-05 at Epoch 5\n",
      "Error: 4.1593129400068155e-05 at Epoch 6\n",
      "Error: 4.1558283632286634e-05 at Epoch 7\n",
      "Error: 4.1525164492208936e-05 at Epoch 8\n",
      "Error: 4.149337952950169e-05 at Epoch 9\n",
      "Error: 4.146269043979902e-05 at Epoch 10\n",
      "Error: 4.1432949115350426e-05 at Epoch 11\n",
      "Error: 4.140405984578324e-05 at Epoch 12\n",
      "Error: 4.137595724337825e-05 at Epoch 13\n",
      "Error: 4.134859344432043e-05 at Epoch 14\n",
      "Error: 4.132193074120469e-05 at Epoch 15\n",
      "Error: 4.129593737654703e-05 at Epoch 16\n",
      "Error: 4.127058516426323e-05 at Epoch 17\n",
      "Error: 4.124584816046767e-05 at Epoch 18\n",
      "Error: 4.1221701931290826e-05 at Epoch 19\n",
      "Error: 4.11981231566126e-05 at Epoch 20\n",
      "Error: 4.117508942007553e-05 at Epoch 21\n",
      "Error: 4.115257910037448e-05 at Epoch 22\n",
      "Error: 4.1130571316067135e-05 at Epoch 23\n",
      "Error: 4.110904589746497e-05 at Epoch 24\n",
      "Error: 4.108798337125831e-05 at Epoch 25\n",
      "Error: 4.1067364950309724e-05 at Epoch 26\n",
      "Error: 4.104717252481425e-05 at Epoch 27\n",
      "Error: 4.102738865305152e-05 at Epoch 28\n",
      "Error: 4.100799655103573e-05 at Epoch 29\n",
      "Error: 4.0988980080908156e-05 at Epoch 30\n",
      "Error: 4.097032373817198e-05 at Epoch 31\n",
      "Error: 4.0952012637970176e-05 at Epoch 32\n",
      "Error: 4.0934032500626124e-05 at Epoch 33\n",
      "Error: 4.0916369636663196e-05 at Epoch 34\n",
      "Error: 4.0899010931479865e-05 at Epoch 35\n",
      "Error: 4.088194382983865e-05 at Epoch 36\n",
      "Error: 4.086515632028484e-05 at Epoch 37\n",
      "Error: 4.084863691959305e-05 at Epoch 38\n",
      "Error: 4.083237465731168e-05 at Epoch 39\n",
      "Error: 4.0816359060461705e-05 at Epoch 40\n",
      "Error: 4.0800580138424735e-05 at Epoch 41\n",
      "Error: 4.0785028368049456e-05 at Epoch 42\n",
      "Error: 4.076969467899173e-05 at Epoch 43\n",
      "Error: 4.0754570439300935e-05 at Epoch 44\n",
      "Error: 4.073964744125224e-05 at Epoch 45\n",
      "Error: 4.0724917887429324e-05 at Epoch 46\n",
      "Error: 4.0710374377050555e-05 at Epoch 47\n",
      "Error: 4.069600989253581e-05 at Epoch 48\n",
      "Error: 4.068181778630698e-05 at Epoch 49\n",
      "Error: 4.0667791767812327e-05 at Epoch 50\n",
      "Error: 4.065392589077058e-05 at Epoch 51\n",
      "Error: 4.064021454062389e-05 at Epoch 52\n",
      "Error: 4.0626652422194225e-05 at Epoch 53\n",
      "Error: 4.061323454753531e-05 at Epoch 54\n",
      "Error: 4.059995622397559e-05 at Epoch 55\n",
      "Error: 4.0586813042345784e-05 at Epoch 56\n",
      "Error: 4.057380086538824e-05 at Epoch 57\n",
      "Error: 4.0560915816345744e-05 at Epoch 58\n",
      "Error: 4.0548154267726007e-05 at Epoch 59\n",
      "Error: 4.053551283024456e-05 at Epoch 60\n",
      "Error: 4.052298834194325e-05 at Epoch 61\n",
      "Error: 4.051057785748951e-05 at Epoch 62\n",
      "Error: 4.049827863765557e-05 at Epoch 63\n",
      "Error: 4.0486088138984675e-05 at Epoch 64\n",
      "Error: 4.0474004003645694e-05 at Epoch 65\n",
      "Error: 4.046202404948396e-05 at Epoch 66\n",
      "Error: 4.045014626027279e-05 at Epoch 67\n",
      "Error: 4.043836877617119e-05 at Epoch 68\n",
      "Error: 4.042668988439817e-05 at Epoch 69\n",
      "Error: 4.041510801012615e-05 at Epoch 70\n",
      "Error: 4.040362170760406e-05 at Epoch 71\n",
      "Error: 4.039222965151754e-05 at Epoch 72\n",
      "Error: 4.0380930628590476e-05 at Epoch 73\n",
      "Error: 4.036972352943942e-05 at Epoch 74\n",
      "Error: 4.035860734068475e-05 at Epoch 75\n",
      "Error: 4.034758113732608e-05 at Epoch 76\n",
      "Error: 4.033664407538866e-05 at Epoch 77\n",
      "Error: 4.032579538484605e-05 at Epoch 78\n",
      "Error: 4.0315034362822717e-05 at Epoch 79\n",
      "Error: 4.030436036708431e-05 at Epoch 80\n",
      "Error: 4.0293772809814123e-05 at Epoch 81\n",
      "Error: 4.028327115168473e-05 at Epoch 82\n",
      "Error: 4.02728548962199e-05 at Epoch 83\n",
      "Error: 4.026252358445414e-05 at Epoch 84\n",
      "Error: 4.025227678988539e-05 at Epoch 85\n",
      "Error: 4.0242114113723433e-05 at Epoch 86\n",
      "Error: 4.023203518042942e-05 at Epoch 87\n",
      "Error: 4.022203963354655e-05 at Epoch 88\n",
      "Error: 4.021212713181763e-05 at Epoch 89\n",
      "Error: 4.0202297345583206e-05 at Epoch 90\n",
      "Error: 4.019254995346018e-05 at Epoch 91\n",
      "Error: 4.018288463928796e-05 at Epoch 92\n",
      "Error: 4.017330108934307e-05 at Epoch 93\n",
      "Error: 4.016379898980915e-05 at Epoch 94\n",
      "Error: 4.015437802449661e-05 at Epoch 95\n",
      "Error: 4.014503787280383e-05 at Epoch 96\n",
      "Error: 4.0135778207909776e-05 at Epoch 97\n",
      "Error: 4.012659869518929e-05 at Epoch 98\n",
      "Error: 4.011749899084151e-05 at Epoch 99\n",
      "Error: 4.0108478740719e-05 at Epoch 100\n",
      "Using Batch Our Neural network Shows that 0.3 + 0.4 = 0.7049477502438257\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stochastic Training of Neural Networks via\n",
    "Successive Convex Approximation\n",
    "'''\n",
    "import numpy as np\n",
    "from random import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class MLP_B():\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,5], num_outputs=3, weight = 0):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights = weight\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        '''\n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1] )\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        # print(\"weights: \", weights)\n",
    "        '''\n",
    "        \n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "        # print(\"activations: \", activations)\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        # print(\"derivatives: \", derivatives)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            # print(\"net inputs:\", net_inputs)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "            # print(\"activations: {} \".format(activations))\n",
    "            # print(\"final activations:\", activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_prop(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            #print(\"activations:\", activations)\n",
    "            # delta:= error.dedrivative_of_activation_function\n",
    "            delta = error * self.tanh_derivative(activations)\n",
    "            #print(\"delta: \", delta)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            #print(\"delta reshaped:\" , delta_reshaped)\n",
    "            current_activations = self.activations[i]\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)\n",
    "            #print(\"current activations:\", current_activations)\n",
    "            #print(\"current activations reshaped\", current_activations_reshaped)\n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            # error := error. derivative_of_activation_function.weights\n",
    "            # delta := error. derivative_of_activation_function\n",
    "            # error := delta.weights\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            #print(error)\n",
    "            #print(\"derivatives:\", self.derivatives[i])\n",
    "        return error\n",
    "\n",
    "    def training(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            #sum_error = 0\n",
    "            batch_size = 100\n",
    "            for step in range(int(len(inputs)/batch_size)):\n",
    "                sum_error = 0\n",
    "                first_index = 0\n",
    "                last_index = batch_size\n",
    "                batch_input = inputs[first_index: last_index]\n",
    "                batch_target = targets[first_index: last_index]\n",
    "\n",
    "                for input, target in zip(batch_input, batch_target):\n",
    "                    output = self.forward(input)\n",
    "                    error = output - target\n",
    "                    self.back_prop(error)\n",
    "                    self.gradient_descent(learning_rate)\n",
    "                    sum_error += self.mean_square_error(target, output)\n",
    "                    \n",
    "                first_index = last_index\n",
    "                last_index += batch_size \n",
    "                #print(\"Error: {} at batch {}\".format(sum_error / batch_size, step+1))\n",
    "               \n",
    "                \n",
    "  \n",
    "            print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "            #plt.plot(sum_error)\n",
    "\n",
    "    def mean_square_error(self, target, output):\n",
    "        return np.average((output - target)**2)\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights -= derivatives * learning_rate \n",
    "\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return (1-x**2)\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp_b = MLP_B(2,[7],1 ,weight )\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp_b.training(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp_b.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"Using Batch Our Neural network Shows that {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Batch Our Neural network Shows that 0.3 + 0.3 = 0.6081831904754302\n"
     ]
    }
   ],
   "source": [
    "pre_input = np.array([0.30,  0.3])\n",
    "prediction_b = mlp_b.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "print(\"With Batch Our Neural network Shows that {} + {} = {}\".format(pre_input[0], pre_input[1], prediction_b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without Batch Our Neural Network Shows: 0.3 + 0.4 = 0.7192884534498386\n",
      "Using Batch Our Neural Network Shows: 0.3 + 0.4 = 0.7048056966314775\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stochastic Training of Neural Networks via\n",
    "Successive Convex Approximation\n",
    "'''\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3,5], num_outputs=3, weight = 0):\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights = weight\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "        \n",
    "        '''\n",
    "        weights = []\n",
    "        for i in range(len(layers)-1):\n",
    "            w = np.random.randn(layers[i] , layers[i+1] )\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "        # print(\"weights: \", weights)\n",
    "        '''\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "        # print(\"activations: \", activations)\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers)-1):\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "        # print(\"derivatives: \", derivatives)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        activations = inputs\n",
    "        self.activations[0] = activations\n",
    "        for i, w in enumerate(self.weights):\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            # print(\"net inputs:\", net_inputs)\n",
    "            activations = self.tanh(net_inputs)\n",
    "            self.activations[i+1] = activations           \n",
    "            # print(\"activations: {} \".format(activations))\n",
    "            # print(\"final activations:\", activations)\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_prop(self, error):\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            #print(\"activations:\", activations)\n",
    "            # delta:= error.dedrivative_of_activation_function\n",
    "            delta = error * self.tanh_derivative(activations)\n",
    "            #print(\"delta: \", delta)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            #print(\"delta reshaped:\" , delta_reshaped)\n",
    "            current_activations = self.activations[i]\n",
    "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)\n",
    "            #print(\"current activations:\", current_activations)\n",
    "            #print(\"current activations reshaped\", current_activations_reshaped)\n",
    "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
    "            # error := error. derivative_of_activation_function.weights\n",
    "            # delta := error. derivative_of_activation_function\n",
    "            # error := delta.weights\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "            #print(error)\n",
    "            #print(\"derivatives:\", self.derivatives[i])\n",
    "        return error\n",
    "\n",
    "    def training(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for input, target in zip(inputs, targets):\n",
    "\n",
    "                output = self.forward(input)\n",
    "                error = output - target\n",
    "                self.back_prop(error)\n",
    "                self.gradient_descent(learning_rate)\n",
    "                sum_error += self.mean_square_error(target, output)\n",
    "            #print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "    \n",
    "    def training_batch(self, inputs, targets, epochs, learning_rate):\n",
    "        for i in range(epochs):\n",
    "            #sum_error = 0\n",
    "            batch_size = 100\n",
    "            for step in range(int(len(inputs)/batch_size)):\n",
    "                sum_error = 0\n",
    "                first_index = 0\n",
    "                last_index = batch_size\n",
    "                batch_input = inputs[first_index: last_index]\n",
    "                batch_target = targets[first_index: last_index]\n",
    "\n",
    "                for input, target in zip(batch_input, batch_target):\n",
    "                    output = self.forward(input)\n",
    "                    error = output - target\n",
    "                    self.back_prop(error)\n",
    "                    self.gradient_descent(learning_rate)\n",
    "                    sum_error += self.mean_square_error(target, output)\n",
    "                    \n",
    "                first_index = last_index\n",
    "                last_index += batch_size \n",
    "                #print(\"Error: {} at batch {}\".format(sum_error / batch_size, step+1))\n",
    "               \n",
    "                \n",
    "  \n",
    "            #print(\"Error: {} at Epoch {}\".format(sum_error / len(inputs), i+1))\n",
    "            #plt.plot(sum_error)\n",
    "\n",
    "    def mean_square_error(self, target, output):\n",
    "        return np.average((output - target)**2)\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights -= derivatives * learning_rate \n",
    "\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return (1-x**2)\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mlp = MLP(2,[7],1, weight)\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp.training(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"without Batch Our Neural Network Shows: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "    mlp_b = MLP(2,[7],1 ,weight )\n",
    "    #inputs = np.random.rand(mlp.num_inputs)\n",
    "    #inputs = np.array([1, 2, 3])\n",
    "    #inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "    targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "    mlp_b.training_batch(inputs, targets, 100, .8)\n",
    "    pre_input = np.array([0.3, 0.4])\n",
    "    prediction = mlp_b.forward(pre_input)\n",
    "    #targets = np.array([6])\n",
    "    print(\"Using Batch Our Neural Network Shows: {} + {} = {}\".format(pre_input[0], pre_input[1], prediction[0]))\n",
    "    #print(\"the network input is: {}\" .format(inputs)) \n",
    "    #print(\"the network output is: {}\" .format(outputs)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Successive Batch for the traing Neural Network the Result \n",
    "#is better than the trainng Neural Network without successive Batch\n",
    "#keeping all other hyperparameters same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
